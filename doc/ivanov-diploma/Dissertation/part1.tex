\chapter{Обзор методов анализа тональности текста}

Для решения задачи анализа тональности существует целый спектр различных подходов. В этой главе мы рассмотрим основные из них.

\section{Методы без учителя}
Самые ранние методы анализа тональности были основаны на эвристиках и предвыбранных словарях слов и работали в основном с неразмеченными данными. Эти методы работают не так хорошо как методы, извлекающие признаки и закономерности из данных, которые будут описаны далее. К примеру, можно  предсказывать тональность прилагательных следующим образом~\cite{Hatzivassiloglou}: сперва строится граф, где вершины - это прилагательные, а рербро являются индикатором того, что два прилагательных, которые оно соединяет, имеют идентичную эмоциональную окраску. Далее для этого графа применяется алгоритм кластеризации, и на основе кластера можно классифицировать прилагательные как позитивные или негативные. Информацию о том, что два прилагательных имеют одинаковую эмоциональную окраску (или противоположную) получается путём анализа того с каким союзом они встречаются вместе: если с союзом 'и', то полагаем, что слова имеют одинаковую окраску, а если с союзом 'но', то - разную.

\section{Методы с учителем}
Методы машинного обучения с учителем находят закономерности в размеченном наборе данных \cite{Pang25}. Некоторый набор признаков автоматически генерируется из предложений. Затем на данных признаках применяются различные виды классификаторов (такие как SVM, наивный байес и другие). Основная работа в данной области сосредоточена на конструировании признаков. К примеру, в \cite{Pang24} экспериментировали с использованием н-грамм, которые являются непрерывными последовательностями из n слов или подстрок. Примерами возможных признаков являются следующие:
\begin{itemize}
    \item Мешок слов - для слова из набора данных мы проверяем содержится ли оно в документе или нет
    \item TF-IDF - для фиксированного слова и документа это величина, учитывающая частоту вхождения слова в документ и частоту появления слова в корпусе документов
    \item Н-граммы на уровне слов - для каждой непрерывной последовательности из n слов проверяем содержится ли она в документе или нет
    \item Н-граммы на уровне символов - для каждой непрерывной последовательности из n символов проверяем содержится ли она в документе или нет
    \item Средний вектор слов, входящих в документ, где векторное представление слов получено алгоритмом Word2Vec или др. на большом наборе неразмеченных данных
    \item Число эмотиконов (актуально для текста из соц. сетей)
    \item Тип последнего символа - является ли последний символ точкой, вопросительным, восклицательным знаком или эмотиконом
\end{itemize}

\section{Тональные словари}
Многие модели машинного обучения включают информацию, полученную из словарей, в их признаки \cite{Jaggi, Mohammad20}. Эти словари являются отображением из слова в величину, характеризующую окраску данного слова. Большинство словарей работают на уровне слов, но они также могут включать биграммы. Словари могут быть получены как вручную \cite{Liu, Mohammad20, Wilson}, так и быть сгенерированны автоматически, используя шумную разметку, полученную автоматически из хештегов или эмотиконов (в случае корпуса сообщений из социальных сетей). Классификатор обучается на юниграммах и биграммах, результатом его работы является отображение из юниграмм и биграмм в оценку близости к каждому из классов тональности \cite{Jaggi, Mohammad19, Severyn30}.

\section{Структурные модели}
Модели описанные выше работают на одном уровне языка: они определяют тональность либо документа, либо предложения, либо фразы, либо слова. Целью структурных моделей является нахождение более сложных аспектов выраженного в тексте мнения, путём учёта нескольких уровней языка. К примеру, автор может критиковать один аспект продукта, но в то же время поощрять другой аспект того же продукта. Т.о. эмоциональная окраска всего отзыва может быть позитивной (уровень документа), но некоторые аспекты отзыва могут быть негативными (уровень предложения или фразы). Эти взаимодействия предложений могут быть смоделированны при помощи графического представления.
  
К примеру, в работе \cite{McDonald} документ смоделирован как ненаправленный граф, где метка каждого предложения зависит от меток предложений по соседству и метки документа. Метка документа зависит от меток предложений. Выходом классификатора т.о. является объединённая метка $y = (y_{d}, y_{s}) = (y_{d}, y_{s_{1}}, ..., y_{s_{n}})$ , где $y_{d}$ представляет собой метку всего документа, а $y_{s_{i}}$ - метка $i$-го предложения. Признаки для предложений $s_{i}$ представлены по средству признакового отображения $f(y_{d}, y_{s_{i-1}} , y_{s_{i+1}}, s_{i})$ . Оно содержит метки документа и предложений по соседству, а также юниграммы, биграммы и триграммы из предложения $s_{i}$. Метки получаются стандартными методами классификации последовательностей.

Другой пример графической модели продемонстрирован в \cite{Wu}, где они представляют тональность как направленный граф. Граф представляет отношения между выражениями мнения (такими как 'хороший', 'плохой' и т.д.) и их модификаторами (например, ’чересчур хороший'), а также отношения между самими выражениями. Этот подход работает на уровне предложения и фразы. Вершины-кандидаты находятся при помощи стандартных методов классификации последовательностей с простыми признаками на уровне символов (например, сам символ, индикаторы того, что это буква, цифра или знак пунктуации, индикатор того, что слово содержится в тональном словаре, индикатор начала или конца слова). Признаки извлекаются на уровне рёбер $f(e _{ij})$ , где $e_{ij}$ есть ребро между вершинами $x_{i}$ и $x_{j}$ , а $f$ есть признаковое отображение. Такой подход решает следующую задачу максимизации: $y = \underset{y}{argmax}\sum_{e_{ij}}\alpha^{T}f(e_{ij})$ где $\alpha$ суть обучаемые параметры модели.

\section{Глубокое обучение}
Нейронные сети уже показали впечатляющие результаты в некоторых приложениях обработки естественного языка. Примерами могут служить семантический анализ \cite{Shen}, машинный перевод \cite{Gao} и анализ тональности \cite{socher-rdm}.

\subsection{Свёрточные нейронные сети}
В частности, свёрточные нейронные сети (CNN) показывают лучшие результаты в некоторых задачах классификации тональности текста \cite{Johnson, Kim, Severyn31}. Основной идей этих методов является использование обучения на размеченных данных сети со свёрточными фильтрами, действующими как скользящее через последовательность слов окно с последующим объединением результатов применения этих фильтров. Часто используются методы векторизации слов такие как \cite{word2vec, Pennington} для представления каждого входного слова. Такие архитектуры нейронных сетей имеют много общего со свёрточными нейронными сетями из компьютерного зрения, которые достигли отличных результатов во многих задачах, например, детекции объектов \cite{Szegedy37} или сегментации изображений \cite{Long}. Свёрточные сети для анализа изображения обычно состоят из гораздо большего числа слоёв, чем их аналоги в приложениях обработки языка \cite{Szegedy37}. 

Более глубокие сети значительно увеличивают число параметров и т.о. требуют большего числа тренировочных примеров. Это является основным препятствием, которое частично объясняет почему глубокие архитектуры в обработке естественного языка имеют очень ограниченное применение. Исключением может служить 6-слойная свёрточная нейронная сеть, работающая на уровне символов, предложенная \cite{Zhang} для классификации текста.

\subsection{Рекуррентные нейронные сети}
Рекуррентные нейронные сети (RNN) ещё раньше продемонстрировали свои возможности в приложениях обработки естественного языка, таких как, например, машинный перевод \cite{Auli, Sutskever, Liu-rnn}.

RNN позволяет работать с последовательностями произвольной длины путём использования рекуррентного скрытого состояния, которое меняется каждый раз при получении очередного элемента последовательности, учитывая предыдущее состояние. И, как правило, данная архитектура превосходит свёрточные нейронные сети в большинстве задач анализа текста~\cite{Yin}. Тем не менее, комбинации свёрточных и рекуррентных слоёв показывают отличные результаты в классификации тональности текста~\cite{Tang, Wang2}.

Также одной из успешных архитектур в задачах классификации текстов стала комбинация рекуррентной нейронной сети с механизмом внимания\cite{bahdanau}.

\subsection{Рекурсивные нейронные сети}
Менее распространённой, но весьма успешной архитектурой является рекурсивная нейронная сеть~\cite{socher-rdm}, использующая дерево разбора предложения. Малое распространение данная архитектура имеет из-за сложной реализации, а также необходимости предвычисления деревьев разбора для всех документов из набора данных. Более того, в работе~\cite{Bowman} показано, что рекуррентная нейронная сеть способна сама извлекать из текста и использовать структурную информацию, которая требуется рекурсивной сети для работы.

\section{Word2Vec}
Нейронные сети для анализа текста нуждаются в качественном векторном представлении поданного входа. Word2Vec \cite{word2vec} основан на нейронной сети, которая создаёт плотное векторное представление для слов. Эти векторы имеют такое свойство, что синтаксически и семантически схожие слова имеют близкие вектора в данном представлении. Word2Vec имеет две основные реализации: continuous bag-of-words (CBOW) и Skip-gram. Модель CBOW предсказывае текущую слово по данному контексту, в то время как модель Skip-
gram предсказывает контекст по слову. Для обоих моделей должна быть задана ширина окна контекста: часто это 3-10 слов по каждую сторону от центрального слова. Пользователь выбирает модель, ширину окна, другие параметры, и модель обучается при помощи градиентного спуска и обратного распространения.
